{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS TO-DO\n",
    "Implement a Monte-Carlo Tree Search Agent that selects actions based on reward to replicate Three Sisters.\n",
    "\n",
    " *Goals*\n",
    "- edit environment so it is not random i.e. agent choose to location of plants\n",
    "- define policy to select state\n",
    "- define how to evalulate a simulation\n",
    "- return best simulation and fine tune\n",
    "\n",
    "*Sunday 2/27*\n",
    "- Define basic agent that uses Adam's value function\n",
    "- Get agent to return best simulation\n",
    "\n",
    "*Monday 2/28* \n",
    "Updates\n",
    "- made agent class\n",
    "- traversal, rollout, backprop code added\n",
    "- state class defined \n",
    "\n",
    "TO-DO\n",
    "- Add tree to agent\n",
    "- Edit environment to reflect precised planting\n",
    "- Finish randomized action based on new environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jordan debug \n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "class MCTS_Agent:\n",
    "\n",
    "    ##root is a Node object that defines the start of the tree to be traversed\n",
    "    def __init__(self, root, field, it, simulations, limit=10):\n",
    "        self.root = root\n",
    "        self.test_bed = field\n",
    "        self.it = it # length of monte carlo trials (i.e. time steps)\n",
    "        self.sow_limit = limit\n",
    "        self.simulations = simulations # number of monte carlo trials\n",
    "\n",
    "    def mcts(self):\n",
    "        for s in range(self.simulations):\n",
    "            leaf = self.traverse(self.root)\n",
    "            simulation_result  = self.rollout(leaf, self.test_bed)\n",
    "            self.backprop(simulation_result)\n",
    "        return self.get_best_path()\n",
    "\n",
    "    # edit this to fit with rest of code -- traverse to best leaf (must be random to build out the tree)\n",
    "    def traverse(self):\n",
    "        for i in range(self.it):\n",
    "            self.test_bed.reset()\n",
    "            node = self.root\n",
    "            path = [] # need to use path\n",
    "\n",
    "            while(node.explored() and not(node.is_terminal())):\n",
    "                best_child = node\n",
    "                best_score = 0\n",
    "                for child in node.children:\n",
    "                    score = child.get_ucb()\n",
    "                    if (score > best_score):\n",
    "                        best_score = score\n",
    "                        best_child = child\n",
    "                path.append(best_child)\n",
    "                best_child.visit()\n",
    "\n",
    "                action = node.get_action()\n",
    "                self.test_bed.step(action)\n",
    "                node = best_child\n",
    "\n",
    "            if (not(node.is_terminal())):\n",
    "                move = node.rand_action() ##What is the maximum dimensions we want to use? Currently setting at 10 x 10\n",
    "                self.test_bed.step(move)\n",
    "                new_node = state(self.test_bed, move, node, False, False)\n",
    "                node.add_child(new_node)\n",
    "                path.append(new_node)\n",
    "                outcome = self.random_sow(new_node)\n",
    "\n",
    "            else:\n",
    "                outcome = node.calc_avg() \n",
    "            self.backprop(path, outcome)\n",
    "\n",
    "    def rollout(self, curr, field): # runs monte carlo simulation by picking random moves from root to terminal, will call backprop()\n",
    "        while not curr.done:\n",
    "            #randomly select an action and make a new state\n",
    "            action = self.rollout_policy()\n",
    "            observation, reward, done, _ = field.step(action) # [self.field, self.calendar], reward, done, {} -- we only need reward\n",
    "            new_node = state(action, curr, reward, done, False) # making a child node\n",
    "            curr.children.append(new_node) # adding child to curr\n",
    "            curr = new_node # moving to child\n",
    "        return curr\n",
    "\n",
    "    #gets random action\n",
    "    #[[choice1, y1, x1], [choice2, y2, x2]...]\n",
    "    def rollout_policy(self):\n",
    "        action = []\n",
    "        for l in range(self.sow_limit):\n",
    "            temp = [random.randint(0,3), np.random.random(), np.random.random()]\n",
    "            action.append(temp)\n",
    "        return action\n",
    "\n",
    "    def backprop(self, curr): # updates heuristic (UCB) and returns the updated path\n",
    "        #if we do node based traversal\n",
    "        while not curr.is_root:\n",
    "            curr.calcUCB()\n",
    "            curr = curr.parent\n",
    "        return \n",
    "\n",
    "    # gets best path\n",
    "    def get_best_path(self):\n",
    "        node = self.root\n",
    "        path = []\n",
    "        while not(len(node.children) == 0):\n",
    "            best_child = node\n",
    "            best_score = 0\n",
    "            for child in node.children:\n",
    "                score = child.get_ucb()\n",
    "                if (score > best_score):\n",
    "                    best_score = score\n",
    "                    best_child = child\n",
    "            path.append(best_child)\n",
    "            node = best_child\n",
    "        return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class state:\n",
    "    def __init__(self, action, pred, reward, term, root):\n",
    "        self.action = action # action on field\n",
    "        self.parent = pred # previous state\n",
    "        self.reward = reward # reward from field\n",
    "        self.terminal =  term # is terminal node\n",
    "        self.children = [] # explored children\n",
    "        self.is_root = root\n",
    "        \n",
    "        # calculating heuristic\n",
    "        self.val = 0 # value of a node i (total yield)\n",
    "        self.avg = 0 # empirical mean of a node i \n",
    "        self.c = 0.1 # constant for UCB: range is 0-1, experiment w different values\n",
    "        self.t = 1 # total number of simulations\n",
    "        # may need to add more data members \n",
    "\n",
    "    def set_term(self, bool):\n",
    "        self.isTerminal = bool\n",
    "\n",
    "    def add_child(self,state):\n",
    "        self.children.append(state)\n",
    "\n",
    "    def get_ucb(self):\n",
    "        if self.is_root:\n",
    "            return self.val/self.t\n",
    "        return self.val/self.t + self.c* math.sqrt(math.log(self.parent.t)/self.t)\n",
    "\n",
    "    def get_reward(self):\n",
    "        return self.val\n",
    "\n",
    "    def get_action(self):\n",
    "        return self.action\n",
    "\n",
    "    def visit(self):\n",
    "        self.t +=1\n",
    "    \n",
    "    def update(self, reward):\n",
    "        self.val += reward\n",
    "\n",
    "    def is_terminal(self):\n",
    "        return self.terminal\n",
    "\n",
    "    def calc_avg(self): # empirical mean of state\n",
    "        return self.val/self.t\n",
    "    \n",
    "    def explored(self):\n",
    "        return len(self.children) == 400\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class MCTS_Agent:\n",
    "\n",
    "    ##root is a Node object that defines the start of the tree to be traversed\n",
    "    def __init__(self, root, field, it, limit=10):\n",
    "        self.root = root\n",
    "        self.test_bed = field\n",
    "        self.it = it\n",
    "        self.sow_limit = limit\n",
    "\n",
    "    ##Run the mcts traversal for the specified number of iterations\n",
    "    def mcts_traversal(self):\n",
    "        for i in range(self.it):\n",
    "            self.test_bed.reset()\n",
    "            node = self.root\n",
    "            path = [] # need to use path\n",
    "\n",
    "            while(node.explored() and not(node.is_terminal())):\n",
    "                best_child = node\n",
    "                best_score = 0\n",
    "                for child in node.children:\n",
    "                    score = child.get_ucb()\n",
    "                    if (score > best_score):\n",
    "                        best_score = score\n",
    "                        best_child = child\n",
    "                path.append(best_child)\n",
    "                best_child.visit()\n",
    "\n",
    "                action = node.get_action()\n",
    "                self.test_bed.step(action)\n",
    "                node = best_child\n",
    "\n",
    "            if (not(node.is_terminal())):\n",
    "                move = node.rand_action() ##What is the maximum dimensions we want to use? Currently setting at 10 x 10\n",
    "                self.test_bed.step(move)\n",
    "                new_node = state(self.test_bed, move, node, False, False)\n",
    "                node.add_child(new_node)\n",
    "                path.append(new_node)\n",
    "                outcome = self.random_sow(new_node)\n",
    "\n",
    "            else:\n",
    "                outcome = node.calc_avg() \n",
    "            self.backprop(path, outcome)\n",
    "\n",
    "    def random_sow(self, start): # runs monte carlo simulation by picking random moves from root to terminal, will call backprop()\n",
    "        curr =  start\n",
    "        done = False\n",
    "        reward = 0\n",
    "        while not(done):\n",
    "            #randomly select an action and make a new state\n",
    "            action = curr.rand_action()\n",
    "            observation, reward, done, _ = self.test_bed.step(action)\n",
    "            new_node = state(observation, action, curr, False, False)\n",
    "            curr = new_node\n",
    "        curr.set_term(True)\n",
    "        return reward\n",
    "\n",
    "\n",
    "    def backprop(self, path, reward): # updates heuristic (UCB) and returns the updated path\n",
    "\n",
    "        for node in path:\n",
    "            node.update(reward)\n",
    "\n",
    "        #if we do node based traversal\n",
    "        # while node.parent:\n",
    "            #node.visit()\n",
    "            #node.calcUCB()\n",
    "            #node = node.parent\n",
    "        return path\n",
    "\n",
    "    ## Should return the best path discovered so far\n",
    "    def get_best_path(self):\n",
    "        path = []\n",
    "        node = self.root\n",
    "        while not(len(node.children) == 0):\n",
    "            best_child = node\n",
    "            best_score = 0\n",
    "            for child in node.children:\n",
    "                score = child.get_ucb()\n",
    "                if (score > best_score):\n",
    "                    best_score = score\n",
    "                    best_child = child\n",
    "            path.append(best_child)\n",
    "            node = best_child\n",
    "        return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edits\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "class MCTS_Agent:\n",
    "\n",
    "    ##root is a Node object that defines the start of the tree to be traversed\n",
    "    def __init__(self, root, field, it, limit=10):\n",
    "        self.root = root\n",
    "        self.test_bed = field\n",
    "        self.it = it\n",
    "        self.sow_limit = limit\n",
    "        self.tree = {}\n",
    "    \n",
    "\n",
    "    ##Run the mcts traversal for the specified number of iterations\n",
    "    def mcts_traversal(self): # search tree\n",
    "        for i in range(self.it):\n",
    "            self.test_bed.reset()\n",
    "            node = self.root\n",
    "            path = [] # need to use path\n",
    "\n",
    "            while(node.explored() and not(node.is_terminal())):\n",
    "                best_child = node\n",
    "                best_score = 0\n",
    "                for child in node.children:\n",
    "                    score = child.get_ucb()\n",
    "                    if (score > best_score):\n",
    "                        best_score = score\n",
    "                        best_child = child\n",
    "                path.append(best_child)\n",
    "                best_child.visit()\n",
    "\n",
    "                action = node.get_action()\n",
    "                self.test_bed.step(action)\n",
    "                node = best_child\n",
    "\n",
    "            if (not(node.is_terminal())):\n",
    "                move = node.rand_action() ##What is the maximum dimensions we want to use? Currently setting at 10 x 10\n",
    "                self.test_bed.step(move)\n",
    "                new_node = state(self.test_bed, move, node, False, False)\n",
    "                node.add_child(new_node)\n",
    "                path.append(new_node)\n",
    "                outcome = self.random_sow(new_node)\n",
    "\n",
    "            else:\n",
    "                outcome = node.calc_avg() \n",
    "            self.backprop(path, outcome)\n",
    "\n",
    "    def random_sow(self, start): # runs monte carlo simulation by picking random moves from root to terminal, will call backprop()\n",
    "        curr =  start\n",
    "        done = False\n",
    "        reward = 0\n",
    "        while not(done):\n",
    "            #randomly select an action and make a new state\n",
    "            action = curr.rand_action()\n",
    "            observation, reward, done, _ = self.test_bed.step(action)\n",
    "            new_node = state(observation, action, curr, False, False)\n",
    "            curr = new_node\n",
    "        curr.set_term(True)\n",
    "        return reward\n",
    "    \n",
    "    def roll_out(self): # builds tree\n",
    "        \n",
    "\n",
    "\n",
    "    def backprop(self, path, reward): # updates heuristic (UCB) and returns the updated path\n",
    "\n",
    "        for node in path:\n",
    "            node.update(reward)\n",
    "\n",
    "        #if we do node based traversal\n",
    "        # while node.parent:\n",
    "            #node.visit()\n",
    "            #node.calcUCB()\n",
    "            #node = node.parent\n",
    "        return path\n",
    "    def rand_action(self): # makes a random action and puts it in tree\n",
    "        action = np.ones((10,3))\n",
    "        for i in range(10):\n",
    "            action[i][0] = np.random.randint(4)\n",
    "            action[i][1] = self.test_bed.size * np.random.random()\n",
    "            action[i][2] = self.test_bed.size * np.random.random()\n",
    "        return action\n",
    "\n",
    "    ## Should return the best path discovered so far\n",
    "    def get_best_path(self):\n",
    "        path = []\n",
    "        node = self.root\n",
    "        while not(len(node.children) == 0):\n",
    "            best_child = node\n",
    "            best_score = 0\n",
    "            for child in node.children:\n",
    "                score = child.get_ucb()\n",
    "                if (score > best_score):\n",
    "                    best_score = score\n",
    "                    best_child = child\n",
    "            path.append(best_child)\n",
    "            node = best_child\n",
    "        return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "from enum import Enum\n",
    "\n",
    "class Plant:\n",
    "    def __init__(self, species, maturity=110):\n",
    "        self.species = species\n",
    "        self.maturity = maturity         # consider 'days_to_maturity'\n",
    "        self.age = 0\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"{}\".format(self.species)\n",
    "    \n",
    "\n",
    "class Field(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, size=5, sow_limit=200, season=120, calendar=0):\n",
    "        # parameters for overall field character\n",
    "        self.size = size\n",
    "        self.sow_limit = sow_limit\n",
    "        self.season = season\n",
    "        self.calendar = calendar\n",
    "        \n",
    "        # constants for computing end-of-season reward---distances represent meters\n",
    "        self.crowding_dist = .02\n",
    "        self.maize_maize_dist = .1\n",
    "        self.bean_support_dist = .1\n",
    "        self.crowding_penalty = .1\n",
    "        self.maize_maize_penalty = .9\n",
    "        self.bean_support_bonus = .6\n",
    "        \n",
    "        # OpenAI action and observation space specifications\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        ### self.observation_space = spaces.???\n",
    "        \n",
    "        # field is initialized by calling reset()\n",
    "        self.field = None\n",
    "        \n",
    "    def step(self, action, yCoord, xCoord):\n",
    "        # sow plants (or wait) depending on actions chosen\n",
    "        # action is an array of n choices; value of n specified in agent code sow_limit\n",
    "        # could be cleaned up with plants as an enumeration?\n",
    "        \n",
    "                   \n",
    "             ## this part of the code is a work in progress!!   \n",
    "             ##------------------START OF WIP------------------------------------   \n",
    "           \n",
    "                \n",
    "             #self.field = np.append(self.field, [[self.size*(coordTuple[0]), self.size*(coordTuple[1]), self.size*(coordTuple[2]), Plant(planttypeTuple[0]), Plant(planttypeTuple[1], Plant(planttypeTuple[2]))])   ,\n",
    "                \n",
    "            \n",
    "            ###Experiment: each choice should be represented as an array with 3 elements:\n",
    "            ### plant choice, y coordinate, x coordinate (in that order).\n",
    "            ### i.e. action should look like: [[choice1, y1, x1], [choice2, y2, x2]...]\n",
    "               \n",
    "     \n",
    "        action = action.reshape(3,3) ##make action into 2 dimensional array\n",
    "\n",
    "        ## L: experimentation so far: not sure if I should add back self.size\n",
    "\n",
    "        for choice in action:\n",
    "            if choice == 0:\n",
    "                self.field = np.append(self.field, [[Plant('Maize'), yCoord, xCoord]], axis=0)\n",
    "                action[choice] = np.append(action, [[\"Maize\", yCoord, xCoord]], axis = 0)\n",
    "            elif choice == 1:\n",
    "                self.field = np.append(self.field, [[Plant('Bean'), yCoord, xCoord]], axis=0)\n",
    "                action[choice] = np.append(action, [[\"Bean\", yCoord, xCoord]], axis = 0)\n",
    "            elif choice == 2:\n",
    "                self.field = np.append(self.field, [[Plant('Squash'), yCoord, xCoord]], axis=0)\n",
    "                action[choice] = np.append(action, [[\"Squash\", yCoord, xCoord]], axis = 0)\n",
    "            # when choice == 3, nothing is done (agent waits)   \n",
    "\n",
    "             ##--------------------------END OF WIP----------------------------------   \n",
    "                                                    \n",
    "     #         for choice in action:   \n",
    "     #             if choice == 0:   \n",
    "     #                 self.field = np.append(self.field, [[self.size * coordTuple,    \n",
    "     #                                              self.size * coordTuple,    \n",
    "     #                                              Plant('Maize')]], axis=0)   \n",
    "        \n",
    "     #             elif choice == 1:   \n",
    "     #                 self.field = np.append(self.field, [[self.size * input(),    \n",
    "     #                                              self.size * input(),    \n",
    "     #                                              Plant('Bean')]], axis=0)   \n",
    "     #             elif choice == 2:   ,\n",
    "     #                 self.field = np.append(self.field, [[self.size * input(),    \n",
    "     #                                              self.size * input(),    \n",
    "     #                                              Plant('Squash')]], axis=0)   \n",
    "            \n",
    "        \n",
    "        # increment timekeeping\n",
    "        self.calendar +=1\n",
    "        for plant in self.field:\n",
    "            plant[2].age += 1\n",
    "            \n",
    "        done = self.calendar == self.season\n",
    "            \n",
    "        if not done:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = self.get_reward()\n",
    "            \n",
    "        return self.field, reward, done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        # field is initialized with one random corn plant in order to make sowing (by np.append) work\n",
    "        self.field = np.array([[self.size * np.random.random(), \n",
    "                                self.size * np.random.random(), \n",
    "                                Plant('Maize')]])\n",
    "        # timekeeping is reset\n",
    "        self.calendar = 0\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        # initialize plant type arrays so that pyplot won't break if any is empty\n",
    "        maize = np.array([[None, None]])\n",
    "        bean = np.array([[None, None]])\n",
    "        squash = np.array([[None, None]])\n",
    "        maize_imm = np.array([[None, None]])\n",
    "        bean_imm = np.array([[None, None]])\n",
    "        squash_imm = np.array([[None, None]])\n",
    "        \n",
    "        # replace initial arrays with coordinates for each plant type; imm are plants that haven't matured\n",
    "        maize = np.array([row for row in self.field \n",
    "                             if row[2].__repr__() == 'Maize' and row[2].age >= row[2].maturity])\n",
    "        bean = np.array([row for row in self.field \n",
    "                            if row[2].__repr__() == 'Bean' and row[2].age >= row[2].maturity])\n",
    "        squash = np.array([row for row in self.field \n",
    "                              if row[2].__repr__() == 'Squash' and row[2].age >= row[2].maturity])\n",
    "        maize_imm = np.array([row for row in self.field \n",
    "                             if row[2].__repr__() == 'Maize' and row[2].age < row[2].maturity])\n",
    "        bean_imm = np.array([row for row in self.field \n",
    "                             if row[2].__repr__() == 'Bean' and row[2].age < row[2].maturity])\n",
    "        squash_imm = np.array([row for row in self.field \n",
    "                             if row[2].__repr__() == 'Squash' and row[2].age < row[2].maturity])\n",
    "        \n",
    "        # plot the field---currently breaks if any plant type is absent\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.scatter(maize[:,0], maize[:,1], c='green', s=200, marker = 'o', alpha=.5, edgecolor='#303030')\n",
    "        plt.scatter(bean[:,0], bean[:,1], c='brown', s=150, marker = 'o', alpha=.5, edgecolor='#303030')\n",
    "        plt.scatter(squash[:,0], squash[:,1], c='orange', s=400, marker = 'o', alpha=.5, edgecolor='#303030')\n",
    "        plt.scatter(maize_imm[:,0], maize_imm[:,1], c='green', s=200, marker = 'o', alpha=.1, edgecolor='#303030')\n",
    "        plt.scatter(bean_imm[:,0], bean_imm[:,1], c='brown', s=200, marker = 'o', alpha=.1, edgecolor='#303030')\n",
    "        plt.scatter(squash_imm[:,0], squash_imm[:,1], c='orange', s=200, marker = 'o', alpha=.1, edgecolor='#303030')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Total yield in Calories is {}.\\n---\\n\".format(round(reward, 1)))\n",
    "    \n",
    "    def close(self):\n",
    "        # unneeded right now? AFAICT this is only used to shut down realtime movie visualizations\n",
    "        pass\n",
    "    \n",
    "    def get_reward(self):\n",
    "        # array of plant coordinates for computing distances\n",
    "        xy_array = np.array([[row[0], row[1]] for row in self.field])\n",
    "\n",
    "        # distances[m,n] is distance from mth to nth plant in field\n",
    "        distances = np.linalg.norm(xy_array - xy_array[:,None], axis=-1)\n",
    "        \n",
    "        reward = 0\n",
    "        i = 0\n",
    "        while i < len(self.field):\n",
    "            if self.field[i,2].age < self.field[i,2].maturity:\n",
    "                reward += 0\n",
    "            elif self.field[i,2].__repr__() == 'Maize':\n",
    "                cal = 1\n",
    "                j = 0\n",
    "                while j < len(distances[0]):\n",
    "                    if (self.field[j,2].__repr__() == 'Bean' \n",
    "                            and distances[i,j] < self.bean_support_dist):\n",
    "                        cal += self.bean_support_bonus\n",
    "                    if (self.field[j,2].__repr__() == 'Maize' \n",
    "                            and i !=j \n",
    "                            and distances[i,j] < self.maize_maize_dist):\n",
    "                        cal *= self.maize_maize_penalty\n",
    "                    if 0 < distances[i,j] < self.crowding_dist:\n",
    "                        cal *= self.crowding_penalty\n",
    "                    j += 1\n",
    "                reward += cal\n",
    "            elif self.field[i,2].__repr__() == 'Bean':\n",
    "                reward += .25\n",
    "            elif self.field[i,2].__repr__() == 'Squash':\n",
    "                reward += 3\n",
    "            i += 1        \n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<__main__.state object at 0x000001DD5F0D3850>, <__main__.state object at 0x000001DD5F0DE280>]\n"
     ]
    }
   ],
   "source": [
    "field = Field()\n",
    "field.reset()\n",
    "test = MCTS_Agent(state(field, [], None, False, True), field, 500)\n",
    "test.mcts_traversal()\n",
    "\n",
    "actions = test.get_best_path()\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-817d435df37a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_agent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMCTS_Agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtest_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmcts_traversal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-493a72eebeda>\u001b[0m in \u001b[0;36mmcts_traversal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m                 \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_child\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_node\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_node\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m                 \u001b[0moutcome\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_sow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_node\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-493a72eebeda>\u001b[0m in \u001b[0;36mrandom_sow\u001b[1;34m(self, start)\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[1;31m#randomly select an action and make a new state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_bed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m             \u001b[0mnew_node\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0mcurr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_node\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-d831a087fc5c>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfield\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-d831a087fc5c>\u001b[0m in \u001b[0;36mget_reward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[1;31m# distances[m,n] is distance from mth to nth plant in field\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxy_array\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mxy_array\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mnorm\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36mnorm\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2558\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mord\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mord\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2559\u001b[0m             \u001b[1;31m# special case for speedup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2560\u001b[1;33m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2561\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2562\u001b[0m         \u001b[1;31m# None of the str-type keywords for ord ('fro', 'nuc')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "field = Field()\n",
    "field.reset()\n",
    "\n",
    "test_agent = MCTS_Agent(state(field, [], None, False, True), field, 10000)\n",
    "test_agent.mcts_traversal()\n",
    "\n",
    "\n",
    "print(test_agent)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
